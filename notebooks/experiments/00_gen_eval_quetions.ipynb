{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30670598-4b28-4e64-ba16-156035a6e648",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc2c061e-ac25-4481-bf98-87bd35997eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from src.mle import utils as mle_utils\n",
    "from src import tags, constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a3d87e6-7410-48dc-927b-a5fdc1d4612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"ai-papers\"\n",
    "\n",
    "path_data_eval_qs = mle_utils.path_data_raw / \"eval-questions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a2d996a-a9aa-405f-bdee-caa2aa841141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe following pdf document is a paper about artificial intelligence.\\n1. Examine the paper and get a view of the overall structure, most important ideas, innovations and base work\\n2. For each section, skip references or contributors,  summarize most important ideas, the overall summary should take into account topics like:\\n    * Main goal or objective of the paper and team invoved\\n    * Novelties introduced by the paper\\n    * Model architecture\\n    * Training procedure, resources and time\\n    * Hiperatarmeter tunning and ablation studies\\n    * Datasets used to train and evaluate\\n    * Infraestructure, number of GPUs, etc., CO2 signature\\n    * Results, comparisons, benchmarks and results\\n    * Limitations and risks\\n5. Finally, based on the prior analysis sumarization, write down at least 10 interesting question and their related answers and source\\n    * Groud truth: Extract the anwswer as close as its original statement as posible\\n    * Source: The section where the anwswer is obtained, title and number\\nThe set of 10 questions should give a complete overview of the paper, anwsering the topics identifed inthe summarization step\\nAt least 2 questions for each paper should be complex questions whose anwsers spans across different sections\\nSkip questions from references or contributors\\n\\nProvide the output as python code as follows:\\n```\\nOverall structure summary and most important ideas as string\\nList of questions in a dictionary form like this:\\nlist_of_questions: List =         \\n        \"paper\": \"TimesFM\",\\n        \"question\": \"What is TimesFM and what problem does it aim to solve?\",\\n        \"ground_truth\": \"TimeSFM is a decoder-only foundation model for time-series forecasting, designed to provide accurate zero-shot forecasts across various domains without the need for task-specific supervised training.\",\\n        \"source\": \"Abstract, Introduction\"\\n```\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "The following pdf document is a paper about artificial intelligence.\n",
    "1. Examine the paper and get a view of the overall structure, most important ideas, innovations and base work\n",
    "2. For each section, skip references or contributors,  summarize most important ideas, the overall summary should take into account topics like:\n",
    "    * Main goal or objective of the paper and team invoved\n",
    "    * Novelties introduced by the paper\n",
    "    * Model architecture\n",
    "    * Training procedure, resources and time\n",
    "    * Hiperatarmeter tunning and ablation studies\n",
    "    * Datasets used to train and evaluate\n",
    "    * Infraestructure, number of GPUs, etc., CO2 signature\n",
    "    * Results, comparisons, benchmarks and results\n",
    "    * Limitations and risks\n",
    "5. Finally, based on the prior analysis sumarization, write down at least 10 interesting question and their related answers and source\n",
    "    * Groud truth: Extract the anwswer as close as its original statement as posible\n",
    "    * Source: The section where the anwswer is obtained, title and number\n",
    "The set of 10 questions should give a complete overview of the paper, anwsering the topics identifed inthe summarization step\n",
    "At least 2 questions for each paper should be complex questions whose anwsers spans across different sections\n",
    "Skip questions from references or contributors\n",
    "\n",
    "Provide the output as python code as follows:\n",
    "```\n",
    "Overall structure summary and most important ideas as string\n",
    "List of questions in a dictionary form like this:\n",
    "list_of_questions: List =         \n",
    "        \"paper\": \"TimesFM\",\n",
    "        \"question\": \"What is TimesFM and what problem does it aim to solve?\",\n",
    "        \"ground_truth\": \"TimeSFM is a decoder-only foundation model for time-series forecasting, designed to provide accurate zero-shot forecasts across various domains without the need for task-specific supervised training.\",\n",
    "        \"source\": \"Abstract, Introduction\"\n",
    "```\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "622ad8e9-469e-47bf-9527-53779d15fc50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper</th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TimesFM</td>\n",
       "      <td>What is the primary goal of the TimesFM model?</td>\n",
       "      <td>The primary goal of TimesFM is to provide accu...</td>\n",
       "      <td>Abstract, Introduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TimesFM</td>\n",
       "      <td>What novelties does the TimesFM introduce in t...</td>\n",
       "      <td>The main novelties include a patched-decoder s...</td>\n",
       "      <td>Abstract, Introduction, Model Architecture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TimesFM</td>\n",
       "      <td>What datasets were used to train TimesFM, and ...</td>\n",
       "      <td>TimesFM was trained on real-world datasets lik...</td>\n",
       "      <td>Pretraining Details, Section 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TimesFM</td>\n",
       "      <td>What is the core architecture of TimesFM?</td>\n",
       "      <td>TimesFM employs a decoder-only architecture wi...</td>\n",
       "      <td>Model Architecture, Section 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TimesFM</td>\n",
       "      <td>How does TimesFM handle long-horizon forecasts...</td>\n",
       "      <td>TimesFM uses longer output patches during deco...</td>\n",
       "      <td>Model Architecture, Section 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TimesFM</td>\n",
       "      <td>What was the computational infrastructure used...</td>\n",
       "      <td>TimesFM was trained on a pretraining corpus of...</td>\n",
       "      <td>Pretraining Details, Section 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TimesFM</td>\n",
       "      <td>What hyperparameters were crucial in training ...</td>\n",
       "      <td>Important hyperparameters included the input a...</td>\n",
       "      <td>Training, Pretraining Details, Section 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TimesFM</td>\n",
       "      <td>How does TimesFM's performance compare to othe...</td>\n",
       "      <td>TimesFM performs close to or surpasses state-o...</td>\n",
       "      <td>Empirical Results, Section 6.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TimesFM</td>\n",
       "      <td>What are the key limitations of TimesFM?</td>\n",
       "      <td>The key limitations of TimesFM include its sma...</td>\n",
       "      <td>Conclusion, Section 7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TimesFM</td>\n",
       "      <td>What risks or potential issues did the authors...</td>\n",
       "      <td>The authors note challenges related to the lac...</td>\n",
       "      <td>Introduction, Conclusion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TimesFM</td>\n",
       "      <td>How does TimesFM handle varying time granulari...</td>\n",
       "      <td>TimesFM can adapt to varying time granularitie...</td>\n",
       "      <td>Model Architecture, Section 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TimesFM</td>\n",
       "      <td>What impact does the input patch length have o...</td>\n",
       "      <td>A larger input patch length, like 32, improves...</td>\n",
       "      <td>Ablation Studies, Section 6.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TimesFM</td>\n",
       "      <td>How does the output patch length affect long-h...</td>\n",
       "      <td>Longer output patches (e.g., 128) enable fewer...</td>\n",
       "      <td>Ablation Studies, Section 6.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      paper                                           question  \\\n",
       "0   TimesFM     What is the primary goal of the TimesFM model?   \n",
       "1   TimesFM  What novelties does the TimesFM introduce in t...   \n",
       "2   TimesFM  What datasets were used to train TimesFM, and ...   \n",
       "3   TimesFM          What is the core architecture of TimesFM?   \n",
       "4   TimesFM  How does TimesFM handle long-horizon forecasts...   \n",
       "5   TimesFM  What was the computational infrastructure used...   \n",
       "6   TimesFM  What hyperparameters were crucial in training ...   \n",
       "7   TimesFM  How does TimesFM's performance compare to othe...   \n",
       "8   TimesFM           What are the key limitations of TimesFM?   \n",
       "9   TimesFM  What risks or potential issues did the authors...   \n",
       "10  TimesFM  How does TimesFM handle varying time granulari...   \n",
       "11  TimesFM  What impact does the input patch length have o...   \n",
       "12  TimesFM  How does the output patch length affect long-h...   \n",
       "\n",
       "                                         ground_truth  \\\n",
       "0   The primary goal of TimesFM is to provide accu...   \n",
       "1   The main novelties include a patched-decoder s...   \n",
       "2   TimesFM was trained on real-world datasets lik...   \n",
       "3   TimesFM employs a decoder-only architecture wi...   \n",
       "4   TimesFM uses longer output patches during deco...   \n",
       "5   TimesFM was trained on a pretraining corpus of...   \n",
       "6   Important hyperparameters included the input a...   \n",
       "7   TimesFM performs close to or surpasses state-o...   \n",
       "8   The key limitations of TimesFM include its sma...   \n",
       "9   The authors note challenges related to the lac...   \n",
       "10  TimesFM can adapt to varying time granularitie...   \n",
       "11  A larger input patch length, like 32, improves...   \n",
       "12  Longer output patches (e.g., 128) enable fewer...   \n",
       "\n",
       "                                        source  \n",
       "0                       Abstract, Introduction  \n",
       "1   Abstract, Introduction, Model Architecture  \n",
       "2               Pretraining Details, Section 5  \n",
       "3                Model Architecture, Section 4  \n",
       "4                Model Architecture, Section 4  \n",
       "5               Pretraining Details, Section 5  \n",
       "6     Training, Pretraining Details, Section 5  \n",
       "7               Empirical Results, Section 6.1  \n",
       "8                        Conclusion, Section 7  \n",
       "9                     Introduction, Conclusion  \n",
       "10               Model Architecture, Section 4  \n",
       "11               Ablation Studies, Section 6.2  \n",
       "12               Ablation Studies, Section 6.2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Overall structure summary and most important ideas\n",
    "overall_summary = \"\"\"\n",
    "The paper introduces TimesFM, a decoder-only foundation model designed for time-series forecasting. Inspired by advances in NLP foundation models, the team sought to develop a time-series model that offers zero-shot forecasting capabilities across diverse datasets without task-specific training. TimesFM is trained on a mix of synthetic and real-world datasets, including Google Trends and Wiki Pageviews, and employs a patched-decoder attention architecture for long-horizon forecasting. The model demonstrates competitive results compared to state-of-the-art supervised models, highlighting its generalization abilities across unseen datasets. The paper presents the model's architecture, training procedures, datasets, ablation studies, and empirical evaluations on various benchmarks, concluding that TimesFM achieves high accuracy with fewer computational resources.\n",
    "\"\"\"\n",
    "\n",
    "# List of questions in dictionary form\n",
    "list_of_questions = [\n",
    "    {\n",
    "        \"paper\": \"TimesFM\",\n",
    "        \"question\": \"What is the primary goal of the TimesFM model?\",\n",
    "        \"ground_truth\": \"The primary goal of TimesFM is to provide accurate zero-shot forecasting across a variety of domains without the need for task-specific supervised training.\",\n",
    "        \"source\": \"Abstract, Introduction\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"TimesFM\",\n",
    "        \"question\": \"What novelties does the TimesFM introduce in time-series forecasting?\",\n",
    "        \"ground_truth\": \"The main novelties include a patched-decoder style attention model, efficient pre-training on a large corpus of synthetic and real-world time-series data, and the ability to perform zero-shot forecasting across different domains and granularities.\",\n",
    "        \"source\": \"Abstract, Introduction, Model Architecture\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"TimesFM\",\n",
    "        \"question\": \"What datasets were used to train TimesFM, and how were they prepared?\",\n",
    "        \"ground_truth\": \"TimesFM was trained on real-world datasets like Google Trends, Wiki Pageviews, the M4 dataset, Electricity, Traffic datasets, and synthetic datasets generated using ARMA processes, seasonal patterns, trends, and step functions.\",\n",
    "        \"source\": \"Pretraining Details, Section 5\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"TimesFM\",\n",
    "        \"question\": \"What is the core architecture of TimesFM?\",\n",
    "        \"ground_truth\": \"TimesFM employs a decoder-only architecture with patched input tokens. It processes the input time-series into patches and uses residual blocks and stacked transformer layers with multi-head causal self-attention.\",\n",
    "        \"source\": \"Model Architecture, Section 4\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"TimesFM\",\n",
    "        \"question\": \"How does TimesFM handle long-horizon forecasts, and why is it important?\",\n",
    "        \"ground_truth\": \"TimesFM uses longer output patches during decoding to make predictions over extended horizons, reducing the number of autoregressive steps needed. This is important for improving accuracy in long-horizon forecasting tasks.\",\n",
    "        \"source\": \"Model Architecture, Section 4\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"TimesFM\",\n",
    "        \"question\": \"What was the computational infrastructure used for training TimesFM?\",\n",
    "        \"ground_truth\": \"TimesFM was trained on a pretraining corpus of around 100B timepoints using a patched-decoder attention architecture. The model has around 200M parameters.\",\n",
    "        \"source\": \"Pretraining Details, Section 5\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"TimesFM\",\n",
    "        \"question\": \"What hyperparameters were crucial in training the TimesFM model, and how were they tuned?\",\n",
    "        \"ground_truth\": \"Important hyperparameters included the input and output patch lengths, model dimension (1280), number of transformer layers (20), and number of heads (16). A cosine decay learning rate schedule with a peak of 5e-4 was used.\",\n",
    "        \"source\": \"Training, Pretraining Details, Section 5\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"TimesFM\",\n",
    "        \"question\": \"How does TimesFM's performance compare to other state-of-the-art models in zero-shot forecasting?\",\n",
    "        \"ground_truth\": \"TimesFM performs close to or surpasses state-of-the-art supervised models in zero-shot settings across various datasets like Monash, Darts, and ETT. It outperforms llmtime and other deep learning models on several benchmarks.\",\n",
    "        \"source\": \"Empirical Results, Section 6.1\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"TimesFM\",\n",
    "        \"question\": \"What are the key limitations of TimesFM?\",\n",
    "        \"ground_truth\": \"The key limitations of TimesFM include its smaller parameter size and pretraining data size compared to large language models. Additionally, fine-tuning and few-shot capabilities were not extensively explored in this paper.\",\n",
    "        \"source\": \"Conclusion, Section 7\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"TimesFM\", \n",
    "        \"question\": \"What risks or potential issues did the authors identify with TimesFM?\",\n",
    "        \"ground_truth\": \"The authors note challenges related to the lack of vast amounts of publicly available time-series data, making it harder to generalize across more diverse datasets. Further work is needed to understand how the model handles out-of-distribution data.\",\n",
    "        \"source\": \"Introduction, Conclusion\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"TimesFM\",\n",
    "        \"question\": \"How does TimesFM handle varying time granularities and context lengths during inference?\",\n",
    "        \"ground_truth\": \"TimesFM can adapt to varying time granularities and context lengths by using a masked patching strategy during training, allowing the model to handle different temporal resolutions and history lengths at inference time.\",\n",
    "        \"source\": \"Model Architecture, Section 4\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"TimesFM\",\n",
    "        \"question\": \"What impact does the input patch length have on TimesFM's performance?\",\n",
    "        \"ground_truth\": \"A larger input patch length, like 32, improves performance by balancing efficiency and accuracy. Too small a patch length leads to inefficient training, while too large a patch length shifts the model towards an encoder-decoder structure, reducing its flexibility.\",\n",
    "        \"source\": \"Ablation Studies, Section 6.2\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"TimesFM\",\n",
    "        \"question\": \"How does the output patch length affect long-horizon forecasting in TimesFM?\",\n",
    "        \"ground_truth\": \"Longer output patches (e.g., 128) enable fewer autoregressive steps during long-horizon forecasting, which improves the model's ability to make accurate predictions over extended periods.\",\n",
    "        \"source\": \"Ablation Studies, Section 6.2\"\n",
    "    } \n",
    "\n",
    "]\n",
    "\n",
    "df_timesfm = pd.DataFrame(list_of_questions)\n",
    "df_timesfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f101450d-41b2-4de4-b1ba-bdc55343b55f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper</th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mamba: Linear-Time Sequence Modeling with Sele...</td>\n",
       "      <td>What is the main goal of the Mamba paper?</td>\n",
       "      <td>The main goal is to overcome the computational...</td>\n",
       "      <td>Abstract, Introduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mamba: Linear-Time Sequence Modeling with Sele...</td>\n",
       "      <td>What is the Selective State Space Model (SSM) ...</td>\n",
       "      <td>Selective SSMs allow the model parameters to c...</td>\n",
       "      <td>Abstract, Section 1, 3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mamba: Linear-Time Sequence Modeling with Sele...</td>\n",
       "      <td>How does the Mamba architecture differ from Tr...</td>\n",
       "      <td>Mamba does not use attention or MLP blocks lik...</td>\n",
       "      <td>Abstract, Section 1, 3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mamba: Linear-Time Sequence Modeling with Sele...</td>\n",
       "      <td>What datasets were used to train and evaluate ...</td>\n",
       "      <td>Mamba was trained and evaluated on synthetic t...</td>\n",
       "      <td>Section 4.1-4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mamba: Linear-Time Sequence Modeling with Sele...</td>\n",
       "      <td>How does Mamba perform compared to Transformer...</td>\n",
       "      <td>Mamba achieves 5× higher inference throughput ...</td>\n",
       "      <td>Section 4.5, Figure 8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mamba: Linear-Time Sequence Modeling with Sele...</td>\n",
       "      <td>What is the hardware optimization that enables...</td>\n",
       "      <td>Mamba uses a hardware-aware parallel algorithm...</td>\n",
       "      <td>Abstract, Section 3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mamba: Linear-Time Sequence Modeling with Sele...</td>\n",
       "      <td>How does Mamba handle long-context sequences a...</td>\n",
       "      <td>Mamba improves performance on tasks with long ...</td>\n",
       "      <td>Section 4.3, Section 4.1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mamba: Linear-Time Sequence Modeling with Sele...</td>\n",
       "      <td>What were the results of the ablation studies ...</td>\n",
       "      <td>Ablation studies show that selective SSMs (S6)...</td>\n",
       "      <td>Section 4.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mamba: Linear-Time Sequence Modeling with Sele...</td>\n",
       "      <td>What is the environmental impact of training t...</td>\n",
       "      <td>Mamba is designed to be computationally effici...</td>\n",
       "      <td>Section 4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mamba: Linear-Time Sequence Modeling with Sele...</td>\n",
       "      <td>What are the main limitations and risks associ...</td>\n",
       "      <td>While Mamba shows strong performance in variou...</td>\n",
       "      <td>Section 5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               paper  \\\n",
       "0  Mamba: Linear-Time Sequence Modeling with Sele...   \n",
       "1  Mamba: Linear-Time Sequence Modeling with Sele...   \n",
       "2  Mamba: Linear-Time Sequence Modeling with Sele...   \n",
       "3  Mamba: Linear-Time Sequence Modeling with Sele...   \n",
       "4  Mamba: Linear-Time Sequence Modeling with Sele...   \n",
       "5  Mamba: Linear-Time Sequence Modeling with Sele...   \n",
       "6  Mamba: Linear-Time Sequence Modeling with Sele...   \n",
       "7  Mamba: Linear-Time Sequence Modeling with Sele...   \n",
       "8  Mamba: Linear-Time Sequence Modeling with Sele...   \n",
       "9  Mamba: Linear-Time Sequence Modeling with Sele...   \n",
       "\n",
       "                                            question  \\\n",
       "0          What is the main goal of the Mamba paper?   \n",
       "1  What is the Selective State Space Model (SSM) ...   \n",
       "2  How does the Mamba architecture differ from Tr...   \n",
       "3  What datasets were used to train and evaluate ...   \n",
       "4  How does Mamba perform compared to Transformer...   \n",
       "5  What is the hardware optimization that enables...   \n",
       "6  How does Mamba handle long-context sequences a...   \n",
       "7  What were the results of the ablation studies ...   \n",
       "8  What is the environmental impact of training t...   \n",
       "9  What are the main limitations and risks associ...   \n",
       "\n",
       "                                        ground_truth  \\\n",
       "0  The main goal is to overcome the computational...   \n",
       "1  Selective SSMs allow the model parameters to c...   \n",
       "2  Mamba does not use attention or MLP blocks lik...   \n",
       "3  Mamba was trained and evaluated on synthetic t...   \n",
       "4  Mamba achieves 5× higher inference throughput ...   \n",
       "5  Mamba uses a hardware-aware parallel algorithm...   \n",
       "6  Mamba improves performance on tasks with long ...   \n",
       "7  Ablation studies show that selective SSMs (S6)...   \n",
       "8  Mamba is designed to be computationally effici...   \n",
       "9  While Mamba shows strong performance in variou...   \n",
       "\n",
       "                       source  \n",
       "0      Abstract, Introduction  \n",
       "1    Abstract, Section 1, 3.2  \n",
       "2    Abstract, Section 1, 3.4  \n",
       "3             Section 4.1-4.3  \n",
       "4       Section 4.5, Figure 8  \n",
       "5       Abstract, Section 3.3  \n",
       "6  Section 4.3, Section 4.1.2  \n",
       "7                 Section 4.6  \n",
       "8                 Section 4.5  \n",
       "9                   Section 5  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_structure_summary = \"\"\"\n",
    "The paper \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\" introduces a novel approach to sequence modeling. The goal is to overcome the limitations of the Transformer architecture, which dominates current deep learning applications but suffers from inefficiencies in modeling long sequences. The authors propose a new type of model called the Selective State Space Model (SSM), integrated into a simplified architecture named Mamba. This model aims to address both the computational inefficiency of Transformers and the weaknesses of structured state-space models (SSMs), which have previously struggled with discrete data like text.\n",
    "\n",
    "Key innovations include the introduction of selective state space dynamics, where the model parameters change based on input data, enabling the model to remember or forget information dynamically. This results in a flexible model that can process sequences of arbitrary length efficiently. The model uses a hardware-aware parallel algorithm that ensures fast processing on modern GPUs, leading to linear time complexity for long sequence processing.\n",
    "\n",
    "The paper explores the architecture, hardware optimization, empirical performance on multiple domains (e.g., language, audio, genomics), and scalability. It also provides benchmarks against state-of-the-art models like Transformers, showing that Mamba can achieve better or comparable performance with fewer computational resources, even on sequences exceeding one million tokens. Furthermore, the model demonstrates faster inference times and higher throughput, making it suitable for practical applications.\n",
    "\"\"\"\n",
    "\n",
    "list_of_questions = [\n",
    "    {\n",
    "        \"paper\": \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\",\n",
    "        \"question\": \"What is the main goal of the Mamba paper?\",\n",
    "        \"ground_truth\": \"The main goal is to overcome the computational inefficiency of Transformer models when processing long sequences and to improve on the weaknesses of structured state space models (SSMs) in handling discrete data like text.\",\n",
    "        \"source\": \"Abstract, Introduction\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\",\n",
    "        \"question\": \"What is the Selective State Space Model (SSM) and how does it work?\",\n",
    "        \"ground_truth\": \"Selective SSMs allow the model parameters to change dynamically based on the input, enabling the model to remember or forget information depending on the data. This improves the model's ability to selectively propagate relevant information across sequences.\",\n",
    "        \"source\": \"Abstract, Section 1, 3.2\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\",\n",
    "        \"question\": \"How does the Mamba architecture differ from Transformers?\",\n",
    "        \"ground_truth\": \"Mamba does not use attention or MLP blocks like Transformers. Instead, it relies on selective state space models, enabling linear-time scaling in sequence length and eliminating the need to store the entire context during inference.\",\n",
    "        \"source\": \"Abstract, Section 1, 3.4\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\",\n",
    "        \"question\": \"What datasets were used to train and evaluate the Mamba model?\",\n",
    "        \"ground_truth\": \"Mamba was trained and evaluated on synthetic tasks (copying and induction heads), language (The Pile), audio (YouTubeMix), and DNA sequences (HG38 genome).\",\n",
    "        \"source\": \"Section 4.1-4.3\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\",\n",
    "        \"question\": \"How does Mamba perform compared to Transformers in terms of inference speed?\",\n",
    "        \"ground_truth\": \"Mamba achieves 5× higher inference throughput than Transformers of similar size due to its recurrent architecture and lack of a need for caching previous elements.\",\n",
    "        \"source\": \"Section 4.5, Figure 8\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\",\n",
    "        \"question\": \"What is the hardware optimization that enables Mamba to scale efficiently on long sequences?\",\n",
    "        \"ground_truth\": \"Mamba uses a hardware-aware parallel algorithm that avoids materializing the expanded state in GPU memory, enabling efficient processing of long sequences by leveraging memory hierarchies.\",\n",
    "        \"source\": \"Abstract, Section 3.3\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\",\n",
    "        \"question\": \"How does Mamba handle long-context sequences and why is this important?\",\n",
    "        \"ground_truth\": \"Mamba improves performance on tasks with long context sequences, up to million-length tokens. Its selection mechanism allows the model to filter out irrelevant information, making it suitable for long-range dependencies like in DNA and language.\",\n",
    "        \"source\": \"Section 4.3, Section 4.1.2\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\",\n",
    "        \"question\": \"What were the results of the ablation studies on the Mamba model architecture?\",\n",
    "        \"ground_truth\": \"Ablation studies show that selective SSMs (S6) significantly outperform other SSM variants and architectures, highlighting the importance of the selection mechanism for sequence modeling.\",\n",
    "        \"source\": \"Section 4.6\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\",\n",
    "        \"question\": \"What is the environmental impact of training the Mamba model in terms of computational resources?\",\n",
    "        \"ground_truth\": \"Mamba is designed to be computationally efficient, reducing the FLOPs required for long-sequence tasks. Although specific CO2 emissions are not detailed, the model’s linear-time complexity and efficient hardware usage imply reduced environmental impact compared to Transformer models.\",\n",
    "        \"source\": \"Section 4.5\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\",\n",
    "        \"question\": \"What are the main limitations and risks associated with the Mamba model?\",\n",
    "        \"ground_truth\": \"While Mamba shows strong performance in various domains, the paper does not explicitly explore risks such as potential biases in pretraining datasets or limitations in handling very short sequences.\",\n",
    "        \"source\": \"Section 5\"\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "df_mamba = pd.DataFrame(list_of_questions)\n",
    "df_mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ab652e6-1b60-4ec0-8486-029dfcde955c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper</th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lag-Llama</td>\n",
       "      <td>What is the main objective of Lag-Llama?</td>\n",
       "      <td>The main objective of Lag-Llama is to develop ...</td>\n",
       "      <td>Abstract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lag-Llama</td>\n",
       "      <td>What are the novelties introduced by the Lag-L...</td>\n",
       "      <td>The novelties include the use of a decoder-onl...</td>\n",
       "      <td>Abstract, Section 1: Introduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lag-Llama</td>\n",
       "      <td>What architecture is used in Lag-Llama, and ho...</td>\n",
       "      <td>Lag-Llama is based on a decoder-only transform...</td>\n",
       "      <td>Section 4: Lag-Llama Architecture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lag-Llama</td>\n",
       "      <td>How was the Lag-Llama model pretrained, and wh...</td>\n",
       "      <td>The model was pretrained on a large corpus of ...</td>\n",
       "      <td>Section 5.1: Datasets, Section 5.3: Hyperparam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lag-Llama</td>\n",
       "      <td>What is the choice of the distribution head us...</td>\n",
       "      <td>The model uses a Student's t-distribution head...</td>\n",
       "      <td>Section 4.3: Choice of Distribution Head</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lag-Llama</td>\n",
       "      <td>What datasets were used to train and evaluate ...</td>\n",
       "      <td>The model was trained on 27 datasets across do...</td>\n",
       "      <td>Section 5.1: Datasets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lag-Llama</td>\n",
       "      <td>What infrastructure was used for the Lag-Llama...</td>\n",
       "      <td>Lag-Llama was trained on a single Nvidia Tesla...</td>\n",
       "      <td>Section 5.3: Hyperparameter Search and Model T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Lag-Llama</td>\n",
       "      <td>How does Lag-Llama perform compared to other s...</td>\n",
       "      <td>Lag-Llama outperforms various statistical and ...</td>\n",
       "      <td>Section 6: Results</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lag-Llama</td>\n",
       "      <td>What are the limitations of the Lag-Llama model?</td>\n",
       "      <td>The limitations include its focus on univariat...</td>\n",
       "      <td>Section 8: Discussion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lag-Llama</td>\n",
       "      <td>How does the Lag-Llama model adapt in few-shot...</td>\n",
       "      <td>Lag-Llama demonstrates strong few-shot adaptat...</td>\n",
       "      <td>Section 6.2: Few-Shot Adaptation Performance o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       paper                                           question  \\\n",
       "0  Lag-Llama           What is the main objective of Lag-Llama?   \n",
       "1  Lag-Llama  What are the novelties introduced by the Lag-L...   \n",
       "2  Lag-Llama  What architecture is used in Lag-Llama, and ho...   \n",
       "3  Lag-Llama  How was the Lag-Llama model pretrained, and wh...   \n",
       "4  Lag-Llama  What is the choice of the distribution head us...   \n",
       "5  Lag-Llama  What datasets were used to train and evaluate ...   \n",
       "6  Lag-Llama  What infrastructure was used for the Lag-Llama...   \n",
       "7  Lag-Llama  How does Lag-Llama perform compared to other s...   \n",
       "8  Lag-Llama   What are the limitations of the Lag-Llama model?   \n",
       "9  Lag-Llama  How does the Lag-Llama model adapt in few-shot...   \n",
       "\n",
       "                                        ground_truth  \\\n",
       "0  The main objective of Lag-Llama is to develop ...   \n",
       "1  The novelties include the use of a decoder-onl...   \n",
       "2  Lag-Llama is based on a decoder-only transform...   \n",
       "3  The model was pretrained on a large corpus of ...   \n",
       "4  The model uses a Student's t-distribution head...   \n",
       "5  The model was trained on 27 datasets across do...   \n",
       "6  Lag-Llama was trained on a single Nvidia Tesla...   \n",
       "7  Lag-Llama outperforms various statistical and ...   \n",
       "8  The limitations include its focus on univariat...   \n",
       "9  Lag-Llama demonstrates strong few-shot adaptat...   \n",
       "\n",
       "                                              source  \n",
       "0                                           Abstract  \n",
       "1                  Abstract, Section 1: Introduction  \n",
       "2                  Section 4: Lag-Llama Architecture  \n",
       "3  Section 5.1: Datasets, Section 5.3: Hyperparam...  \n",
       "4           Section 4.3: Choice of Distribution Head  \n",
       "5                              Section 5.1: Datasets  \n",
       "6  Section 5.3: Hyperparameter Search and Model T...  \n",
       "7                                 Section 6: Results  \n",
       "8                              Section 8: Discussion  \n",
       "9  Section 6.2: Few-Shot Adaptation Performance o...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Overall structure summary and most important ideas as string\n",
    "overall_structure_summary = \"\"\"\n",
    "The paper \"Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting\" presents a new approach to univariate probabilistic time series forecasting by introducing a foundation model called Lag-Llama. This model is based on a decoder-only transformer architecture and uses lag features and covariates to handle time series data from various domains. The goal is to achieve strong zero-shot generalization performance and state-of-the-art results without relying on task-specific models.\n",
    "\n",
    "Key innovations include:\n",
    "- The use of lag features as covariates in a decoder-only transformer architecture.\n",
    "- Pretraining on a large corpus of time series data across several domains.\n",
    "- Strong zero-shot performance on unseen datasets and state-of-the-art performance after fine-tuning on smaller datasets.\n",
    "- Robust few-shot adaptation capabilities, making the model highly adaptable to various levels of historical data.\n",
    "\n",
    "The paper explores the scalability of the model and the diversity of the pretraining corpus. It compares Lag-Llama's performance against several statistical and deep learning models, showing that Lag-Llama outperforms competitors, especially after fine-tuning.\n",
    "\n",
    "The infrastructure used for training involves a single Nvidia Tesla-P100 GPU, with 100 hyperparameter configurations tested during training. The datasets used are from diverse domains such as energy, economics, and transportation. The model's performance is evaluated using the Continuous Ranked Probability Score (CRPS).\n",
    "\n",
    "Limitations of the paper include the model's focus on univariate data, leaving multivariate extensions and more complex distribution heads for future work. Furthermore, the study suggests the need for large-scale time series datasets and highlights the challenges in adapting foundation models for time series data.\n",
    "\"\"\"\n",
    "\n",
    "# List of questions and answers in dictionary form\n",
    "list_of_questions = [\n",
    "    {\n",
    "        \"paper\": \"Lag-Llama\",\n",
    "        \"question\": \"What is the main objective of Lag-Llama?\",\n",
    "        \"ground_truth\": \"The main objective of Lag-Llama is to develop a general-purpose foundation model for univariate probabilistic time series forecasting, with strong zero-shot performance and adaptability across different domains, without task-specific training.\",\n",
    "        \"source\": \"Abstract\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"Lag-Llama\",\n",
    "        \"question\": \"What are the novelties introduced by the Lag-Llama model?\",\n",
    "        \"ground_truth\": \"The novelties include the use of a decoder-only transformer architecture with lag features as covariates and its capability to perform zero-shot forecasting across various time series domains.\",\n",
    "        \"source\": \"Abstract, Section 1: Introduction\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"Lag-Llama\",\n",
    "        \"question\": \"What architecture is used in Lag-Llama, and how does it process the time series data?\",\n",
    "        \"ground_truth\": \"Lag-Llama is based on a decoder-only transformer architecture. It tokenizes time series data using lagged features and covariates, and passes them through multiple layers of transformers with pre-normalization and Rotary Positional Encoding (RoPE).\",\n",
    "        \"source\": \"Section 4: Lag-Llama Architecture\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"Lag-Llama\",\n",
    "        \"question\": \"How was the Lag-Llama model pretrained, and what was the training setup?\",\n",
    "        \"ground_truth\": \"The model was pretrained on a large corpus of 27 time series datasets across six domains, comprising around 352 million data windows (tokens). The training batch size was 256, with a learning rate of 10^-4, and early stopping based on validation loss.\",\n",
    "        \"source\": \"Section 5.1: Datasets, Section 5.3: Hyperparameter Search and Model Training Setups\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"Lag-Llama\",\n",
    "        \"question\": \"What is the choice of the distribution head used in Lag-Llama?\",\n",
    "        \"ground_truth\": \"The model uses a Student's t-distribution head, which outputs the parameters of the distribution, including degrees of freedom, mean, and scale. The choice was made to keep the model simple and effective for training.\",\n",
    "        \"source\": \"Section 4.3: Choice of Distribution Head\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"Lag-Llama\",\n",
    "        \"question\": \"What datasets were used to train and evaluate the Lag-Llama model?\",\n",
    "        \"ground_truth\": \"The model was trained on 27 datasets across domains like energy, economics, transportation, nature, and air quality. Datasets were selected to test the model's generalization capabilities across diverse prediction horizons.\",\n",
    "        \"source\": \"Section 5.1: Datasets\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"Lag-Llama\",\n",
    "        \"question\": \"What infrastructure was used for the Lag-Llama model's training, and what resources were required?\",\n",
    "        \"ground_truth\": \"Lag-Llama was trained on a single Nvidia Tesla-P100 GPU with 12 GB of memory, 4 CPU cores, and 24 GB of RAM. The training involved 100 hyperparameter configurations and large-scale pretraining.\",\n",
    "        \"source\": \"Section 5.3: Hyperparameter Search and Model Training Setups\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"Lag-Llama\",\n",
    "        \"question\": \"How does Lag-Llama perform compared to other state-of-the-art models?\",\n",
    "        \"ground_truth\": \"Lag-Llama outperforms various statistical and deep learning models, particularly in zero-shot and fine-tuning scenarios. After fine-tuning, it consistently achieves state-of-the-art performance across diverse datasets.\",\n",
    "        \"source\": \"Section 6: Results\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"Lag-Llama\",\n",
    "        \"question\": \"What are the limitations of the Lag-Llama model?\",\n",
    "        \"ground_truth\": \"The limitations include its focus on univariate time series, with multivariate extensions and more complex distribution heads left for future work. The need for large-scale time series datasets is also highlighted.\",\n",
    "        \"source\": \"Section 8: Discussion\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"Lag-Llama\",\n",
    "        \"question\": \"How does the Lag-Llama model adapt in few-shot scenarios with limited historical data?\",\n",
    "        \"ground_truth\": \"Lag-Llama demonstrates strong few-shot adaptation capabilities, showing increasing performance as more historical data becomes available. It outperforms baseline models even when limited history is available.\",\n",
    "        \"source\": \"Section 6.2: Few-Shot Adaptation Performance on Unseen Data\"\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "df_lagllama = pd.DataFrame(list_of_questions)\n",
    "df_lagllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "685f8ded-01d8-4f0a-977c-ecbfbbd0e2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper</th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ColPali</td>\n",
       "      <td>What problem does ColPali aim to solve in docu...</td>\n",
       "      <td>ColPali addresses the inefficiencies in curren...</td>\n",
       "      <td>Abstract, Introduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ColPali</td>\n",
       "      <td>What is the main architectural innovation intr...</td>\n",
       "      <td>ColPali leverages Vision Language Models to in...</td>\n",
       "      <td>Section 2, Problem Formulation &amp; Related Work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ColPali</td>\n",
       "      <td>What is the ViDoRe benchmark, and why was it i...</td>\n",
       "      <td>ViDoRe is a benchmark specifically designed to...</td>\n",
       "      <td>Section 3, The ViDoRe Benchmark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ColPali</td>\n",
       "      <td>What datasets were used to train the ColPali m...</td>\n",
       "      <td>The model was trained on 127,460 query-page pa...</td>\n",
       "      <td>Section 4.2, Model training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ColPali</td>\n",
       "      <td>What training infrastructure was used for ColP...</td>\n",
       "      <td>ColPali was trained on an 8-GPU setup with dat...</td>\n",
       "      <td>Section 4.2, Model training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ColPali</td>\n",
       "      <td>What were the main performance results of ColP...</td>\n",
       "      <td>ColPali outperformed all evaluated models on t...</td>\n",
       "      <td>Section 5.1, Results</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ColPali</td>\n",
       "      <td>How does ColPali achieve faster indexing and q...</td>\n",
       "      <td>ColPali directly encodes pages from their imag...</td>\n",
       "      <td>Section 5.2, Latencies &amp; Memory Footprint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ColPali</td>\n",
       "      <td>What ablation studies were conducted, and what...</td>\n",
       "      <td>Ablation studies on patch number and query aug...</td>\n",
       "      <td>Section 6, Ablation study</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ColPali</td>\n",
       "      <td>What are the main limitations of the ColPali m...</td>\n",
       "      <td>ColPali primarily focuses on PDF-type document...</td>\n",
       "      <td>Section 7, Limitations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ColPali</td>\n",
       "      <td>What environmental impact did the training of ...</td>\n",
       "      <td>The experiments consumed 1405 Mi250x GPU hours...</td>\n",
       "      <td>Section 7, Ethical Considerations</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     paper                                           question  \\\n",
       "0  ColPali  What problem does ColPali aim to solve in docu...   \n",
       "1  ColPali  What is the main architectural innovation intr...   \n",
       "2  ColPali  What is the ViDoRe benchmark, and why was it i...   \n",
       "3  ColPali  What datasets were used to train the ColPali m...   \n",
       "4  ColPali  What training infrastructure was used for ColP...   \n",
       "5  ColPali  What were the main performance results of ColP...   \n",
       "6  ColPali  How does ColPali achieve faster indexing and q...   \n",
       "7  ColPali  What ablation studies were conducted, and what...   \n",
       "8  ColPali  What are the main limitations of the ColPali m...   \n",
       "9  ColPali  What environmental impact did the training of ...   \n",
       "\n",
       "                                        ground_truth  \\\n",
       "0  ColPali addresses the inefficiencies in curren...   \n",
       "1  ColPali leverages Vision Language Models to in...   \n",
       "2  ViDoRe is a benchmark specifically designed to...   \n",
       "3  The model was trained on 127,460 query-page pa...   \n",
       "4  ColPali was trained on an 8-GPU setup with dat...   \n",
       "5  ColPali outperformed all evaluated models on t...   \n",
       "6  ColPali directly encodes pages from their imag...   \n",
       "7  Ablation studies on patch number and query aug...   \n",
       "8  ColPali primarily focuses on PDF-type document...   \n",
       "9  The experiments consumed 1405 Mi250x GPU hours...   \n",
       "\n",
       "                                          source  \n",
       "0                         Abstract, Introduction  \n",
       "1  Section 2, Problem Formulation & Related Work  \n",
       "2                Section 3, The ViDoRe Benchmark  \n",
       "3                    Section 4.2, Model training  \n",
       "4                    Section 4.2, Model training  \n",
       "5                           Section 5.1, Results  \n",
       "6      Section 5.2, Latencies & Memory Footprint  \n",
       "7                      Section 6, Ablation study  \n",
       "8                         Section 7, Limitations  \n",
       "9              Section 7, Ethical Considerations  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Overall_structure_summary = \"\"\"\n",
    "The paper \"ColPali: Efficient Document Retrieval with Vision Language Models\" introduces ColPali, a novel retrieval system designed to optimize document retrieval through both visual and textual cues. It seeks to address inefficiencies in modern retrieval systems, particularly in handling visually rich documents that combine text with images, tables, and other visual elements. \n",
    "\n",
    "The paper is structured as follows:\n",
    "1. **Abstract and Introduction**: The main goal is to improve retrieval in visually rich documents by combining text and vision-language models. The work highlights the importance of using both text and visual elements for better document matching, with ColPali being introduced as an efficient alternative.\n",
    "2. **Problem Formulation and Related Work**: The paper contrasts ColPali's design with existing retrieval methods, emphasizing the importance of late interaction mechanisms for processing multi-modal documents.\n",
    "3. **ViDoRe Benchmark**: Introduces the Visual Document Retrieval Benchmark (ViDoRe), used to test ColPali’s performance. This section outlines a comprehensive dataset covering various document types, domains, and languages.\n",
    "4. **Architecture**: Describes the ColPali model architecture, which integrates visual embeddings with text embeddings using a late interaction mechanism.\n",
    "5. **Training Procedure**: Discusses how the ColPali model was trained, including datasets, hyperparameters, and infrastructure used.\n",
    "6. **Results and Comparisons**: ColPali outperforms baseline models on several benchmarks in both performance and latency, especially in visually complex tasks like tables and figures.\n",
    "7. **Ablation Studies**: Investigates various design choices, such as the number of image patches or vision component fine-tuning, and their impact on performance.\n",
    "8. **Conclusions**: The paper concludes by highlighting ColPali’s superior performance, flexibility, and future avenues for combining retrieval with visual question answering.\n",
    "9. **Limitations and Ethical Considerations**: Discusses potential biases and the need for more diverse document types and languages in future studies.\n",
    "\n",
    "Key Innovations:\n",
    "- A late interaction model architecture specifically designed for visually rich document retrieval.\n",
    "- ViDoRe benchmark, which evaluates retrieval across diverse document formats and languages.\n",
    "- Superior indexing speed and low query latency, allowing fast and accurate document retrieval.\n",
    "\n",
    "Datasets and Infrastructure:\n",
    "- The paper mentions training ColPali on 127,460 query-page pairs.\n",
    "- Uses an 8-GPU setup with LoRA adapters for efficiency.\n",
    "- Estimates a carbon footprint of around 15 kg CO2 eq. for the experiments.\n",
    "\n",
    "Results:\n",
    "- ColPali outperforms state-of-the-art methods on the ViDoRe benchmark, particularly in tasks involving infographics and tables.\n",
    "- Demonstrates faster indexing and querying times compared to traditional methods.\n",
    "- Significant improvements in complex document retrieval tasks, especially with late interaction mechanisms.\n",
    "\n",
    "Limitations:\n",
    "- Focus on PDF documents with less emphasis on web or handwritten content.\n",
    "- Generalization to lower-resource languages remains untested.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "list_of_questions: List = [\n",
    "    {\n",
    "        \"paper\": \"ColPali\",\n",
    "        \"question\": \"What problem does ColPali aim to solve in document retrieval?\",\n",
    "        \"ground_truth\": \"ColPali addresses the inefficiencies in current document retrieval systems, particularly in handling visually rich documents that contain text, tables, images, and layouts, which are often overlooked by modern text-centric systems.\",\n",
    "        \"source\": \"Abstract, Introduction\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"ColPali\",\n",
    "        \"question\": \"What is the main architectural innovation introduced by ColPali?\",\n",
    "        \"ground_truth\": \"ColPali leverages Vision Language Models to index documents based on visual features and integrates a late interaction mechanism to enhance query matching.\",\n",
    "        \"source\": \"Section 2, Problem Formulation & Related Work\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"ColPali\",\n",
    "        \"question\": \"What is the ViDoRe benchmark, and why was it introduced?\",\n",
    "        \"ground_truth\": \"ViDoRe is a benchmark specifically designed to evaluate document retrieval systems on visually rich documents, considering both textual and visual elements across multiple domains and languages.\",\n",
    "        \"source\": \"Section 3, The ViDoRe Benchmark\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"ColPali\",\n",
    "        \"question\": \"What datasets were used to train the ColPali model?\",\n",
    "        \"ground_truth\": \"The model was trained on 127,460 query-page pairs, with 63% coming from openly available academic datasets and 37% from synthetic datasets composed of web-crawled PDFs and VLM-generated pseudo-questions.\",\n",
    "        \"source\": \"Section 4.2, Model training\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"ColPali\",\n",
    "        \"question\": \"What training infrastructure was used for ColPali?\",\n",
    "        \"ground_truth\": \"ColPali was trained on an 8-GPU setup with data parallelism, LoRA adapters (r=32), and paged_adamw_8bit optimizer.\",\n",
    "        \"source\": \"Section 4.2, Model training\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"ColPali\",\n",
    "        \"question\": \"What were the main performance results of ColPali on the ViDoRe benchmark?\",\n",
    "        \"ground_truth\": \"ColPali outperformed all evaluated models on the ViDoRe benchmark, particularly excelling in visually complex tasks like InfographicVQA, ArxivQA, and TabFQuAD, with significantly higher NDCG@5 scores.\",\n",
    "        \"source\": \"Section 5.1, Results\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"ColPali\",\n",
    "        \"question\": \"How does ColPali achieve faster indexing and querying times compared to other systems?\",\n",
    "        \"ground_truth\": \"ColPali directly encodes pages from their image representation, bypassing time-consuming steps like layout detection, OCR, and chunking, resulting in faster indexing times.\",\n",
    "        \"source\": \"Section 5.2, Latencies & Memory Footprint\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"ColPali\",\n",
    "        \"question\": \"What ablation studies were conducted, and what were the findings inColPali paper?\",\n",
    "        \"ground_truth\": \"Ablation studies on patch number and query augmentation tokens showed trade-offs between performance and memory usage, with 1024 patches yielding the best results. Query augmentation tokens had a marginal impact on English but improved performance in French tasks.\",\n",
    "        \"source\": \"Section 6, Ablation study\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"ColPali\",\n",
    "        \"question\": \"What are the main limitations of the ColPali model?\",\n",
    "        \"ground_truth\": \"ColPali primarily focuses on PDF-type documents and high-resource languages, with limited evaluation on web screenshots or low-resource languages.\",\n",
    "        \"source\": \"Section 7, Limitations\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"ColPali\",\n",
    "        \"question\": \"What environmental impact did the training of ColPali have?\",\n",
    "        \"ground_truth\": \"The experiments consumed 1405 Mi250x GPU hours, resulting in an estimated carbon footprint of around 15kg CO2 eq, using low-carbon nuclear energy.\",\n",
    "        \"source\": \"Section 7, Ethical Considerations\"\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "df_colpali = pd.DataFrame(list_of_questions)\n",
    "df_colpali\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58aea5f4-2d00-4202-bc95-91a5f01cd6f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper</th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PaliGemma</td>\n",
       "      <td>What is PaliGemma and what problem does it aim...</td>\n",
       "      <td>PaliGemma is an open Vision-Language Model (VL...</td>\n",
       "      <td>Introduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PaliGemma</td>\n",
       "      <td>What are the key components of PaliGemma’s arc...</td>\n",
       "      <td>PaliGemma consists of three components: the Si...</td>\n",
       "      <td>Model Architecture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PaliGemma</td>\n",
       "      <td>How does PaliGemma handle different image reso...</td>\n",
       "      <td>PaliGemma undergoes Stage 2 training to increa...</td>\n",
       "      <td>Training Procedure, Section 3.2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PaliGemma</td>\n",
       "      <td>What datasets were used to train and evaluate ...</td>\n",
       "      <td>PaliGemma was trained on a broad mixture of mu...</td>\n",
       "      <td>Training Procedure, Section 3.2; Results, Sect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PaliGemma</td>\n",
       "      <td>How does PaliGemma perform compared to larger ...</td>\n",
       "      <td>PaliGemma, with less than 3B parameters, achie...</td>\n",
       "      <td>Introduction, Section 1; Results, Section 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PaliGemma</td>\n",
       "      <td>What pretraining stages does PaliGemma undergo...</td>\n",
       "      <td>PaliGemma undergoes three stages: unimodal pre...</td>\n",
       "      <td>Training Procedure, Section 3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PaliGemma</td>\n",
       "      <td>What infrastructure was used to train PaliGemm...</td>\n",
       "      <td>PaliGemma was trained on Cloud TPUv5e, with St...</td>\n",
       "      <td>Training Infrastructure, Section 3.2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PaliGemma</td>\n",
       "      <td>What ablation studies were conducted, and what...</td>\n",
       "      <td>Ablation studies showed that freezing the visi...</td>\n",
       "      <td>Ablation Studies, Section 5.1; Section 5.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PaliGemma</td>\n",
       "      <td>What are PaliGemma's main limitations?</td>\n",
       "      <td>The main limitations include the lack of instr...</td>\n",
       "      <td>Limitations, Section 8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PaliGemma</td>\n",
       "      <td>How does PaliGemma achieve transferability acr...</td>\n",
       "      <td>PaliGemma uses a flexible fine-tuning approach...</td>\n",
       "      <td>Transferability, Section 6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       paper                                           question  \\\n",
       "0  PaliGemma  What is PaliGemma and what problem does it aim...   \n",
       "1  PaliGemma  What are the key components of PaliGemma’s arc...   \n",
       "2  PaliGemma  How does PaliGemma handle different image reso...   \n",
       "3  PaliGemma  What datasets were used to train and evaluate ...   \n",
       "4  PaliGemma  How does PaliGemma perform compared to larger ...   \n",
       "5  PaliGemma  What pretraining stages does PaliGemma undergo...   \n",
       "6  PaliGemma  What infrastructure was used to train PaliGemm...   \n",
       "7  PaliGemma  What ablation studies were conducted, and what...   \n",
       "8  PaliGemma             What are PaliGemma's main limitations?   \n",
       "9  PaliGemma  How does PaliGemma achieve transferability acr...   \n",
       "\n",
       "                                        ground_truth  \\\n",
       "0  PaliGemma is an open Vision-Language Model (VL...   \n",
       "1  PaliGemma consists of three components: the Si...   \n",
       "2  PaliGemma undergoes Stage 2 training to increa...   \n",
       "3  PaliGemma was trained on a broad mixture of mu...   \n",
       "4  PaliGemma, with less than 3B parameters, achie...   \n",
       "5  PaliGemma undergoes three stages: unimodal pre...   \n",
       "6  PaliGemma was trained on Cloud TPUv5e, with St...   \n",
       "7  Ablation studies showed that freezing the visi...   \n",
       "8  The main limitations include the lack of instr...   \n",
       "9  PaliGemma uses a flexible fine-tuning approach...   \n",
       "\n",
       "                                              source  \n",
       "0                                       Introduction  \n",
       "1                                 Model Architecture  \n",
       "2                  Training Procedure, Section 3.2.3  \n",
       "3  Training Procedure, Section 3.2; Results, Sect...  \n",
       "4        Introduction, Section 1; Results, Section 4  \n",
       "5                    Training Procedure, Section 3.2  \n",
       "6             Training Infrastructure, Section 3.2.6  \n",
       "7         Ablation Studies, Section 5.1; Section 5.7  \n",
       "8                             Limitations, Section 8  \n",
       "9                         Transferability, Section 6  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Overall_structure_summary = \"\"\"\n",
    "The paper \"PaliGemma: A versatile 3B VLM for transfer\" introduces PaliGemma, a versatile vision-language model (VLM) with 3 billion parameters. The primary goal of this work is to provide a flexible and transferable model for a wide range of vision-language tasks, including image captioning, visual question answering (VQA), segmentation, and many more. The model architecture combines a vision encoder (SigLIP-So400m) and a language decoder (Gemma-2B). The paper evaluates PaliGemma on almost 40 tasks and highlights its performance across a broad spectrum of domains and benchmarks.\n",
    "\n",
    "The paper is structured as follows:\n",
    "1. **Introduction**: The paper introduces PaliGemma, explaining its foundation in the PaLI and Gemma models. The objective is to create a model that is both powerful and transferable, able to perform well on standard VLM tasks and specialized areas like remote sensing and video captioning.\n",
    "2. **Related Work**: This section discusses prior work in VLMs, including notable models like CLIP, ALIGN, and PaLI, and how they paved the way for PaliGemma.\n",
    "3. **Model Architecture**: The PaliGemma model consists of three main components: the SigLIP vision encoder, the Gemma-2B language model, and a linear projection layer connecting the vision and language models. The architecture supports various multimodal tasks, leveraging a simple image+text input and generating text outputs for different applications.\n",
    "4. **Training Procedure**: The training process follows three main stages: unimodal pretraining (using existing off-the-shelf models), multimodal pretraining (combining vision and language models), and a resolution increase phase to enable the model to process higher-resolution images.\n",
    "5. **Results**: PaliGemma is tested on over 30 benchmarks, including tasks like image captioning, visual question answering, and segmentation. The model demonstrates state-of-the-art performance on tasks requiring higher resolution and specialized vision-language capabilities.\n",
    "6. **Ablation Studies**: This section explores various model design choices, such as the effectiveness of freezing components during pretraining and the impact of higher resolution on performance.\n",
    "7. **Transferability**: The paper examines the model's transferability across tasks, highlighting its flexibility with minimal fine-tuning.\n",
    "8. **Conclusion**: PaliGemma is presented as a robust, open, and flexible VLM, offering strong performance across a wide range of vision-language tasks, with the potential for future research in instruction tuning and further specialized applications.\n",
    "\n",
    "Key Innovations:\n",
    "- Combines the SigLIP vision encoder and the Gemma-2B language model into a powerful yet efficient VLM.\n",
    "- Introduces multimodal pretraining strategies and increases image resolution during training for high-resolution tasks.\n",
    "- Outperforms larger models on many benchmarks while being smaller in parameter count.\n",
    "\n",
    "Datasets and Infrastructure:\n",
    "- The training is conducted on TPUv5e with 256 cores, with the training phase taking about 3 days for Stage 1 and 15 hours for Stage 2.\n",
    "- The model sees 1 billion multimodal examples during pretraining and is further fine-tuned on specialized tasks.\n",
    "\n",
    "Results:\n",
    "- PaliGemma achieves strong results on benchmarks like COCO Captions, VQA v2, and Remote Sensing VQA, demonstrating that smaller models can achieve performance on par with much larger VLMs.\n",
    "\n",
    "Limitations:\n",
    "- The model lacks instruction tuning, which could improve zero-shot generalization.\n",
    "- Potential biases in the pretraining data due to the use of publicly available datasets.\n",
    "\"\"\"\n",
    "\n",
    "list_of_questions: List = [\n",
    "    {\n",
    "        \"paper\": \"PaliGemma\",\n",
    "        \"question\": \"What is PaliGemma and what problem does it aim to solve?\",\n",
    "        \"ground_truth\": \"PaliGemma is an open Vision-Language Model (VLM) that combines the SigLIP-So400m vision encoder and the Gemma-2B language model, aiming to provide a versatile base model for a wide range of vision-language tasks with efficient transferability.\",\n",
    "        \"source\": \"Introduction\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"PaliGemma\",\n",
    "        \"question\": \"What are the key components of PaliGemma’s architecture?\",\n",
    "        \"ground_truth\": \"PaliGemma consists of three components: the SigLIP image encoder, the Gemma-2B language decoder, and a linear projection layer that connects the vision and language models.\",\n",
    "        \"source\": \"Model Architecture\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"PaliGemma\",\n",
    "        \"question\": \"How does PaliGemma handle different image resolutions during training and evaluation?\",\n",
    "        \"ground_truth\": \"PaliGemma undergoes Stage 2 training to increase the image resolution from 224px to 448px and 896px, enhancing its performance on tasks requiring high-resolution inputs like detection and segmentation.\",\n",
    "        \"source\": \"Training Procedure, Section 3.2.3\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"PaliGemma\",\n",
    "        \"question\": \"What datasets were used to train and evaluate PaliGemma?\",\n",
    "        \"ground_truth\": \"PaliGemma was trained on a broad mixture of multimodal tasks, and it was evaluated on over 30 benchmarks, including tasks like COCO Captions, VQA, and Remote Sensing VQA.\",\n",
    "        \"source\": \"Training Procedure, Section 3.2; Results, Section 4\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"PaliGemma\",\n",
    "        \"question\": \"How does PaliGemma perform compared to larger VLM models?\",\n",
    "        \"ground_truth\": \"PaliGemma, with less than 3B parameters, achieves performance on par with much larger models like PaLI-X (55B) and PaLM-E (562B), particularly on benchmarks like ScienceQA and VQA v2.\",\n",
    "        \"source\": \"Introduction, Section 1; Results, Section 4\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"PaliGemma\",\n",
    "        \"question\": \"What pretraining stages does PaliGemma undergo, and what is the purpose of each stage?\",\n",
    "        \"ground_truth\": \"PaliGemma undergoes three stages: unimodal pretraining using off-the-shelf models, multimodal pretraining for task alignment, and a resolution increase stage to improve high-resolution task performance.\",\n",
    "        \"source\": \"Training Procedure, Section 3.2\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"PaliGemma\",\n",
    "        \"question\": \"What infrastructure was used to train PaliGemma, and how long did the training take?\",\n",
    "        \"ground_truth\": \"PaliGemma was trained on Cloud TPUv5e, with Stage 1 pretraining taking 3 days and Stage 2 lasting 15 hours.\",\n",
    "        \"source\": \"Training Infrastructure, Section 3.2.6\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"PaliGemma\",\n",
    "        \"question\": \"What ablation studies were conducted, and what were the findings in PaliGemma paper?\",\n",
    "        \"ground_truth\": \"Ablation studies showed that freezing the vision encoder during pretraining degraded performance on spatial tasks, and increasing image resolution significantly boosted performance on tasks requiring high-resolution input.\",\n",
    "        \"source\": \"Ablation Studies, Section 5.1; Section 5.7\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"PaliGemma\",\n",
    "        \"question\": \"What are PaliGemma's main limitations?\",\n",
    "        \"ground_truth\": \"The main limitations include the lack of instruction tuning, which affects zero-shot generalization, and potential biases in the training data derived from publicly available datasets.\",\n",
    "        \"source\": \"Limitations, Section 8\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"PaliGemma\",\n",
    "        \"question\": \"How does PaliGemma achieve transferability across different tasks?\",\n",
    "        \"ground_truth\": \"PaliGemma uses a flexible fine-tuning approach and transferability recipe, where a few hyperparameters like resolution, learning rate, and epochs are adapted for different tasks, allowing it to transfer effectively across a wide range of benchmarks.\",\n",
    "        \"source\": \"Transferability, Section 6\"\n",
    "    }\n",
    "]\n",
    "\n",
    "df_paligemma = pd.DataFrame(list_of_questions)\n",
    "df_paligemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f385c58e-875b-4c37-bb6f-7ddaa950d16e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper</th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Llama 3</td>\n",
       "      <td>What is Llama 3 and what is the primary goal o...</td>\n",
       "      <td>Llama 3 is a herd of language models designed ...</td>\n",
       "      <td>Introduction, Section 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Llama 3</td>\n",
       "      <td>What are the main components of Llama 3’s mode...</td>\n",
       "      <td>Llama 3 adopts a dense Transformer architectur...</td>\n",
       "      <td>Model Architecture, Section 3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Llama 3</td>\n",
       "      <td>How was Llama 3 trained, and what infrastructu...</td>\n",
       "      <td>Llama 3 was trained on a corpus of 15.6T token...</td>\n",
       "      <td>Training Procedure, Section 3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama 3</td>\n",
       "      <td>What datasets were used for pretraining Llama ...</td>\n",
       "      <td>The pretraining dataset for Llama 3 includes 1...</td>\n",
       "      <td>Pre-Training Data, Section 3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Llama 3</td>\n",
       "      <td>What performance benchmarks were used to evalu...</td>\n",
       "      <td>Llama 3 was evaluated on benchmarks such as MM...</td>\n",
       "      <td>Results, Section 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Llama 3</td>\n",
       "      <td>How does the Llama 3 model handle long-context...</td>\n",
       "      <td>Llama 3 is designed to handle long contexts, s...</td>\n",
       "      <td>Long Context Pre-Training, Section 3.4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Llama 3</td>\n",
       "      <td>What is Grouped Query Attention (GQA) and why ...</td>\n",
       "      <td>GQA is an attention mechanism used in Llama 3 ...</td>\n",
       "      <td>Model Architecture, Section 3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Llama 3</td>\n",
       "      <td>What strategies were used to scale the Llama 3...</td>\n",
       "      <td>Llama 3 uses 4D parallelism, combining tensor,...</td>\n",
       "      <td>Infrastructure, Section 3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Llama 3</td>\n",
       "      <td>What ablation studies were conducted on Llama ...</td>\n",
       "      <td>Ablation studies focused on hyperparameters li...</td>\n",
       "      <td>Ablation Studies, Section 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Llama 3</td>\n",
       "      <td>What are the limitations of Llama 3, and what ...</td>\n",
       "      <td>The limitations of Llama 3 include challenges ...</td>\n",
       "      <td>Limitations and Future Directions, Section 6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     paper                                           question  \\\n",
       "0  Llama 3  What is Llama 3 and what is the primary goal o...   \n",
       "1  Llama 3  What are the main components of Llama 3’s mode...   \n",
       "2  Llama 3  How was Llama 3 trained, and what infrastructu...   \n",
       "3  Llama 3  What datasets were used for pretraining Llama ...   \n",
       "4  Llama 3  What performance benchmarks were used to evalu...   \n",
       "5  Llama 3  How does the Llama 3 model handle long-context...   \n",
       "6  Llama 3  What is Grouped Query Attention (GQA) and why ...   \n",
       "7  Llama 3  What strategies were used to scale the Llama 3...   \n",
       "8  Llama 3  What ablation studies were conducted on Llama ...   \n",
       "9  Llama 3  What are the limitations of Llama 3, and what ...   \n",
       "\n",
       "                                        ground_truth  \\\n",
       "0  Llama 3 is a herd of language models designed ...   \n",
       "1  Llama 3 adopts a dense Transformer architectur...   \n",
       "2  Llama 3 was trained on a corpus of 15.6T token...   \n",
       "3  The pretraining dataset for Llama 3 includes 1...   \n",
       "4  Llama 3 was evaluated on benchmarks such as MM...   \n",
       "5  Llama 3 is designed to handle long contexts, s...   \n",
       "6  GQA is an attention mechanism used in Llama 3 ...   \n",
       "7  Llama 3 uses 4D parallelism, combining tensor,...   \n",
       "8  Ablation studies focused on hyperparameters li...   \n",
       "9  The limitations of Llama 3 include challenges ...   \n",
       "\n",
       "                                         source  \n",
       "0                       Introduction, Section 1  \n",
       "1               Model Architecture, Section 3.2  \n",
       "2               Training Procedure, Section 3.3  \n",
       "3                Pre-Training Data, Section 3.1  \n",
       "4                            Results, Section 4  \n",
       "5      Long Context Pre-Training, Section 3.4.2  \n",
       "6               Model Architecture, Section 3.2  \n",
       "7                   Infrastructure, Section 3.3  \n",
       "8                   Ablation Studies, Section 5  \n",
       "9  Limitations and Future Directions, Section 6  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Overall_structure_summary = \"\"\"\n",
    "The paper titled \"Llama 3: Herd of Models\" presents Llama 3, a new set of foundation models designed for language tasks and multimodal AI tasks, developed by Meta's AI team. Llama 3 models natively support multilinguality, reasoning, coding, and tool use, with a flagship model boasting 405B parameters and a context window of up to 128K tokens. The primary goal of this paper is to introduce Llama 3 and evaluate its performance across a variety of benchmarks, showing that it performs comparably with top models like GPT-4.\n",
    "\n",
    "The paper structure is as follows:\n",
    "1. **Introduction**: Llama 3's core contribution is its scale and optimization in data, multilinguality, coding, and reasoning tasks. The model outperforms many competitors, demonstrating significant improvements over its predecessors, Llama and Llama 2.\n",
    "2. **Model Architecture**: Llama 3 adopts a dense Transformer architecture with some modifications such as grouped query attention (GQA) and a larger vocabulary to improve efficiency, multilinguality, and long-context capabilities. The flagship model has 405B parameters and supports 128K tokens in context.\n",
    "3. **Training Procedure**: The training process for Llama 3 involved large-scale pretraining on 15.6T tokens, with an emphasis on multilingual and diverse data sources. The models were trained on Meta's AI infrastructure using up to 16K H100 GPUs.\n",
    "4. **Scaling Laws**: Extensive use of scaling laws guided the development and optimization of Llama 3, ensuring a compute-optimal balance between model size and training data.\n",
    "5. **Results and Benchmarks**: Llama 3 performs competitively across multiple benchmarks like MMLU, GSM8K, HumanEval, and ARC Challenge, achieving state-of-the-art results in several categories. The model also excels in multilingual tasks.\n",
    "6. **Ablation Studies**: Various experiments were conducted to analyze the effects of different hyperparameters, including the impact of increased sequence length, GQA, and long-context performance.\n",
    "7. **Infrastructure**: Llama 3's pretraining was performed on Meta’s AI supercluster using 16K GPUs. The compute budget was 3.8x10^25 FLOPs, and the model was trained on 15.6T tokens. Efficient parallelism strategies like 4D parallelism were used to optimize memory and speed.\n",
    "8. **Limitations and Future Directions**: The paper highlights limitations, such as challenges in scaling beyond 405B parameters, potential biases in pretraining data, and areas for future research including instruction tuning and better multimodal integration.\n",
    "\n",
    "Key Innovations:\n",
    "- **Grouped Query Attention (GQA)**: Improved inference speed and memory efficiency.\n",
    "- **Long-context Handling**: Extended to 128K tokens, enhancing reasoning and document-processing tasks.\n",
    "- **Multimodal Extensions**: Initial efforts to integrate image, video, and speech capabilities.\n",
    "\n",
    "Datasets and Infrastructure:\n",
    "- Llama 3 was trained on 15.6 trillion tokens of text data and required significant computational resources, involving up to 16,000 H100 GPUs for training.\n",
    "\n",
    "Results:\n",
    "- Llama 3 achieves state-of-the-art performance in several benchmarks, outperforming its predecessors and other models with comparable parameter sizes. It also demonstrates strong generalization across languages.\n",
    "\n",
    "Limitations:\n",
    "- Further tuning is needed for better zero-shot performance and instruction-following capabilities. Moreover, biases in pretraining data could influence model behavior.\n",
    "\"\"\"\n",
    "\n",
    "list_of_questions: List = [\n",
    "    {\n",
    "        \"paper\": \"Llama 3\",\n",
    "        \"question\": \"What is Llama 3 and what is the primary goal of the paper?\",\n",
    "        \"ground_truth\": \"Llama 3 is a herd of language models designed to support multilinguality, reasoning, coding, and tool use. The goal of the paper is to present Llama 3 and demonstrate its performance across benchmarks, showing comparable quality to GPT-4.\",\n",
    "        \"source\": \"Introduction, Section 1\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"Llama 3\",\n",
    "        \"question\": \"What are the main components of Llama 3’s model architecture?\",\n",
    "        \"ground_truth\": \"Llama 3 adopts a dense Transformer architecture with 405B parameters. The architecture includes grouped query attention (GQA) for improved inference speed and larger vocabulary for better token efficiency.\",\n",
    "        \"source\": \"Model Architecture, Section 3.2\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"Llama 3\",\n",
    "        \"question\": \"How was Llama 3 trained, and what infrastructure was used?\",\n",
    "        \"ground_truth\": \"Llama 3 was trained on a corpus of 15.6T tokens using up to 16K H100 GPUs with a compute budget of 3.8×10^25 FLOPs. Training took place on Meta’s AI production clusters with optimized parallelism strategies.\",\n",
    "        \"source\": \"Training Procedure, Section 3.3\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"Llama 3\",\n",
    "        \"question\": \"What datasets were used for pretraining Llama 3, and how was data quality ensured?\",\n",
    "        \"ground_truth\": \"The pretraining dataset for Llama 3 includes 15.6T tokens from a mixture of multilingual, reasoning, mathematical, and code data. Data quality was ensured through aggressive de-duplication and filtering, particularly for web-based content.\",\n",
    "        \"source\": \"Pre-Training Data, Section 3.1\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"Llama 3\",\n",
    "        \"question\": \"What performance benchmarks were used to evaluate Llama 3?\",\n",
    "        \"ground_truth\": \"Llama 3 was evaluated on benchmarks such as MMLU, GSM8K, HumanEval, ARC Challenge, and multilingual tasks. The flagship model achieved state-of-the-art performance in several benchmarks.\",\n",
    "        \"source\": \"Results, Section 4\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"Llama 3\",\n",
    "        \"question\": \"How does the Llama 3 model handle long-context processing?\",\n",
    "        \"ground_truth\": \"Llama 3 is designed to handle long contexts, supporting up to 128K tokens. This feature is crucial for tasks that require understanding or generating long documents, improving performance on 'needle in a haystack' tasks.\",\n",
    "        \"source\": \"Long Context Pre-Training, Section 3.4.2\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"Llama 3\",\n",
    "        \"question\": \"What is Grouped Query Attention (GQA) and why is it significant in Llama 3?\",\n",
    "        \"ground_truth\": \"GQA is an attention mechanism used in Llama 3 with 8 key-value heads, which improves inference speed and reduces memory requirements, making the model more efficient during decoding.\",\n",
    "        \"source\": \"Model Architecture, Section 3.2\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"Llama 3\",\n",
    "        \"question\": \"What strategies were used to scale the Llama 3 model efficiently?\",\n",
    "        \"ground_truth\": \"Llama 3 uses 4D parallelism, combining tensor, pipeline, context, and data parallelism. This approach ensures efficient distribution of computation across thousands of GPUs, optimizing memory usage and performance.\",\n",
    "        \"source\": \"Infrastructure, Section 3.3\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"Llama 3\",\n",
    "        \"question\": \"What ablation studies were conducted on Llama 3 and what were the findings?\",\n",
    "        \"ground_truth\": \"Ablation studies focused on hyperparameters like sequence length and GQA. Findings showed that longer sequences and the use of GQA significantly improved model performance on tasks requiring reasoning and long-context understanding.\",\n",
    "        \"source\": \"Ablation Studies, Section 5\"\n",
    "    },\n",
    "    {\n",
    "        \"paper\": \"Llama 3\",\n",
    "        \"question\": \"What are the limitations of Llama 3, and what future directions are suggested?\",\n",
    "        \"ground_truth\": \"The limitations of Llama 3 include challenges with instruction tuning and potential biases in pretraining data. Future work includes improving multimodal integration and better instruction-following capabilities.\",\n",
    "        \"source\": \"Limitations and Future Directions, Section 6\"\n",
    "    }\n",
    "]\n",
    "\n",
    "df_llama3herd = pd.DataFrame(list_of_questions)\n",
    "df_llama3herd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37d0623a-1f63-4f91-90ce-bc2c179b87ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper</th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>source</th>\n",
       "      <th>split_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mamba: Linear-Time Sequence Modeling with Sele...</td>\n",
       "      <td>What are the main limitations and risks associ...</td>\n",
       "      <td>While Mamba shows strong performance in variou...</td>\n",
       "      <td>Section 5</td>\n",
       "      <td>1.train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PaliGemma</td>\n",
       "      <td>How does PaliGemma perform compared to larger ...</td>\n",
       "      <td>PaliGemma, with less than 3B parameters, achie...</td>\n",
       "      <td>Introduction, Section 1; Results, Section 4</td>\n",
       "      <td>1.train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               paper  \\\n",
       "9  Mamba: Linear-Time Sequence Modeling with Sele...   \n",
       "4                                          PaliGemma   \n",
       "\n",
       "                                            question  \\\n",
       "9  What are the main limitations and risks associ...   \n",
       "4  How does PaliGemma perform compared to larger ...   \n",
       "\n",
       "                                        ground_truth  \\\n",
       "9  While Mamba shows strong performance in variou...   \n",
       "4  PaliGemma, with less than 3B parameters, achie...   \n",
       "\n",
       "                                        source   split_  \n",
       "9                                    Section 5  1.train  \n",
       "4  Introduction, Section 1; Results, Section 4  1.train  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_questions = pd.concat([\n",
    "    df_timesfm,\n",
    "    df_mamba,\n",
    "    df_lagllama,\n",
    "    df_colpali,\n",
    "    df_paligemma,\n",
    "    df_llama3herd\n",
    "], axis=0)\n",
    "\n",
    "df_train_valid, df_test = train_test_split(df_questions, test_size=0.6, random_state= constants.RND_SEED, shuffle=True, stratify=df_questions['paper'])\n",
    "df_train, df_valid = train_test_split(df_train_valid, test_size=0.5, random_state= constants.RND_SEED, shuffle=True, stratify=df_train_valid['paper'])\n",
    "\n",
    "df_questions = pd.concat([\n",
    "    df_train.assign(split_=mle_utils.Splits.TRAIN.value),\n",
    "    df_valid.assign(split_=mle_utils.Splits.VALID.value),\n",
    "    df_test.assign(split_=mle_utils.Splits.TEST.value)\n",
    "], axis=0)\n",
    "\n",
    "\n",
    "df_questions.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5590511-5b77-4ecb-8052-737e4d9714b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions.to_csv(path_data_eval_qs / f\"{index_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "081a71d5-8162-4ff0-ab1a-3104b68e0fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langfuse import Langfuse\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "langfuse = Langfuse()\n",
    "langfuse.auth_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03b1282-a731-468c-a151-89637d0825ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
