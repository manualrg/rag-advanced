{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97a5a3e4-4348-46e9-a793-17c480ab7875",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ba71f2-fdb9-4ab7-8153-3e4296288e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from src.mle import utils as mle_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1af7b5de-adf8-4ece-b5a6-9dc22036b413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\manua\\\\.conda\\\\envs\\\\rag-advanced\\\\python.exe'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f10460f6-4673-4f8c-9c3b-a7f02a1aa44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_src = \"ai-papers\"\n",
    "index_name = \"ai-papers\"\n",
    "llm_emb = \"text-embedding-3-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e188104e-1f1e-4288-8b8e-b99ce80afd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "embeddings = OpenAIEmbeddings(model=llm_emb)\n",
    "\n",
    "path_db = mle_utils.path_data_interm / index_name / \"chroma_langchain_db\"\n",
    "path_db.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "path_corpus = mle_utils.path_data_raw / folder_src "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "adcc1576-6d06-41a5-ab22-1fd11f349de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\users\\manua\\documents\\repos\\dslabs\\rag-advanced\\data\\raw\\ai-papers\\colpali.pdf\n",
      "c:\\users\\manua\\documents\\repos\\dslabs\\rag-advanced\\data\\raw\\ai-papers\\lagllama.pdf\n",
      "c:\\users\\manua\\documents\\repos\\dslabs\\rag-advanced\\data\\raw\\ai-papers\\llama3herd.pdf\n",
      "c:\\users\\manua\\documents\\repos\\dslabs\\rag-advanced\\data\\raw\\ai-papers\\mamba.pdf\n",
      "c:\\users\\manua\\documents\\repos\\dslabs\\rag-advanced\\data\\raw\\ai-papers\\paligemma.pdf\n",
      "c:\\users\\manua\\documents\\repos\\dslabs\\rag-advanced\\data\\raw\\ai-papers\\timesfm.pdf\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "lst_docs_corpus = []\n",
    "uuids = []\n",
    "for file in path_corpus.iterdir():\n",
    "    loader = PyPDFLoader(path_corpus / file)\n",
    "    print(file)\n",
    "    logging.info(\"Parsing file: %s\", file)\n",
    "    docs = loader.load()\n",
    "    for doc in docs:\n",
    "        doc.metadata['file_name'] = file.stem\n",
    "        doc.metadata['extension'] = file.suffix\n",
    "        doc.metadata['page'] += 1\n",
    "        uuids.append(file.stem + \"-\" + str(doc.metadata['page']))\n",
    "    logging.info(\"SUCCESS: Parsed file: %s, obtained %s documents\", file, len(docs))\n",
    "    lst_docs_corpus.extend(docs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3157bf16-1fb5-4b2e-9cd4-99e224e47313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a0b5b13-4396-4dd2-ba35-7c2cad81e5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['colpali-1',\n",
       " 'colpali-2',\n",
       " 'colpali-3',\n",
       " 'colpali-4',\n",
       " 'colpali-5',\n",
       " 'colpali-6',\n",
       " 'colpali-7',\n",
       " 'colpali-8',\n",
       " 'colpali-9',\n",
       " 'colpali-10',\n",
       " 'colpali-11',\n",
       " 'colpali-12',\n",
       " 'colpali-13',\n",
       " 'colpali-14',\n",
       " 'colpali-15',\n",
       " 'colpali-16',\n",
       " 'colpali-17',\n",
       " 'colpali-18',\n",
       " 'colpali-19',\n",
       " 'colpali-20',\n",
       " 'lagllama-1',\n",
       " 'lagllama-2',\n",
       " 'lagllama-3',\n",
       " 'lagllama-4',\n",
       " 'lagllama-5',\n",
       " 'lagllama-6',\n",
       " 'lagllama-7',\n",
       " 'lagllama-8',\n",
       " 'lagllama-9',\n",
       " 'lagllama-10',\n",
       " 'lagllama-11',\n",
       " 'lagllama-12',\n",
       " 'lagllama-13',\n",
       " 'lagllama-14',\n",
       " 'lagllama-15',\n",
       " 'lagllama-16',\n",
       " 'lagllama-17',\n",
       " 'lagllama-18',\n",
       " 'lagllama-19',\n",
       " 'lagllama-20',\n",
       " 'lagllama-21',\n",
       " 'lagllama-22',\n",
       " 'lagllama-23',\n",
       " 'llama3herd-1',\n",
       " 'llama3herd-2',\n",
       " 'llama3herd-3',\n",
       " 'llama3herd-4',\n",
       " 'llama3herd-5',\n",
       " 'llama3herd-6',\n",
       " 'llama3herd-7',\n",
       " 'llama3herd-8',\n",
       " 'llama3herd-9',\n",
       " 'llama3herd-10',\n",
       " 'llama3herd-11',\n",
       " 'llama3herd-12',\n",
       " 'llama3herd-13',\n",
       " 'llama3herd-14',\n",
       " 'llama3herd-15',\n",
       " 'llama3herd-16',\n",
       " 'llama3herd-17',\n",
       " 'llama3herd-18',\n",
       " 'llama3herd-19',\n",
       " 'llama3herd-20',\n",
       " 'llama3herd-21',\n",
       " 'llama3herd-22',\n",
       " 'llama3herd-23',\n",
       " 'llama3herd-24',\n",
       " 'llama3herd-25',\n",
       " 'llama3herd-26',\n",
       " 'llama3herd-27',\n",
       " 'llama3herd-28',\n",
       " 'llama3herd-29',\n",
       " 'llama3herd-30',\n",
       " 'llama3herd-31',\n",
       " 'llama3herd-32',\n",
       " 'llama3herd-33',\n",
       " 'llama3herd-34',\n",
       " 'llama3herd-35',\n",
       " 'llama3herd-36',\n",
       " 'llama3herd-37',\n",
       " 'llama3herd-38',\n",
       " 'llama3herd-39',\n",
       " 'llama3herd-40',\n",
       " 'llama3herd-41',\n",
       " 'llama3herd-42',\n",
       " 'llama3herd-43',\n",
       " 'llama3herd-44',\n",
       " 'llama3herd-45',\n",
       " 'llama3herd-46',\n",
       " 'llama3herd-47',\n",
       " 'llama3herd-48',\n",
       " 'llama3herd-49',\n",
       " 'llama3herd-50',\n",
       " 'llama3herd-51',\n",
       " 'llama3herd-52',\n",
       " 'llama3herd-53',\n",
       " 'llama3herd-54',\n",
       " 'llama3herd-55',\n",
       " 'llama3herd-56',\n",
       " 'llama3herd-57',\n",
       " 'llama3herd-58',\n",
       " 'llama3herd-59',\n",
       " 'llama3herd-60',\n",
       " 'llama3herd-61',\n",
       " 'llama3herd-62',\n",
       " 'llama3herd-63',\n",
       " 'llama3herd-64',\n",
       " 'llama3herd-65',\n",
       " 'llama3herd-66',\n",
       " 'llama3herd-67',\n",
       " 'llama3herd-68',\n",
       " 'llama3herd-69',\n",
       " 'llama3herd-70',\n",
       " 'llama3herd-71',\n",
       " 'llama3herd-72',\n",
       " 'llama3herd-73',\n",
       " 'llama3herd-74',\n",
       " 'llama3herd-75',\n",
       " 'llama3herd-76',\n",
       " 'llama3herd-77',\n",
       " 'llama3herd-78',\n",
       " 'llama3herd-79',\n",
       " 'llama3herd-80',\n",
       " 'llama3herd-81',\n",
       " 'llama3herd-82',\n",
       " 'llama3herd-83',\n",
       " 'llama3herd-84',\n",
       " 'llama3herd-85',\n",
       " 'llama3herd-86',\n",
       " 'llama3herd-87',\n",
       " 'llama3herd-88',\n",
       " 'llama3herd-89',\n",
       " 'llama3herd-90',\n",
       " 'llama3herd-91',\n",
       " 'llama3herd-92',\n",
       " 'mamba-1',\n",
       " 'mamba-2',\n",
       " 'mamba-3',\n",
       " 'mamba-4',\n",
       " 'mamba-5',\n",
       " 'mamba-6',\n",
       " 'mamba-7',\n",
       " 'mamba-8',\n",
       " 'mamba-9',\n",
       " 'mamba-10',\n",
       " 'mamba-11',\n",
       " 'mamba-12',\n",
       " 'mamba-13',\n",
       " 'mamba-14',\n",
       " 'mamba-15',\n",
       " 'mamba-16',\n",
       " 'mamba-17',\n",
       " 'mamba-18',\n",
       " 'mamba-19',\n",
       " 'mamba-20',\n",
       " 'mamba-21',\n",
       " 'mamba-22',\n",
       " 'mamba-23',\n",
       " 'mamba-24',\n",
       " 'mamba-25',\n",
       " 'mamba-26',\n",
       " 'mamba-27',\n",
       " 'mamba-28',\n",
       " 'mamba-29',\n",
       " 'mamba-30',\n",
       " 'mamba-31',\n",
       " 'mamba-32',\n",
       " 'mamba-33',\n",
       " 'mamba-34',\n",
       " 'mamba-35',\n",
       " 'mamba-36',\n",
       " 'paligemma-1',\n",
       " 'paligemma-2',\n",
       " 'paligemma-3',\n",
       " 'paligemma-4',\n",
       " 'paligemma-5',\n",
       " 'paligemma-6',\n",
       " 'paligemma-7',\n",
       " 'paligemma-8',\n",
       " 'paligemma-9',\n",
       " 'paligemma-10',\n",
       " 'paligemma-11',\n",
       " 'paligemma-12',\n",
       " 'paligemma-13',\n",
       " 'paligemma-14',\n",
       " 'paligemma-15',\n",
       " 'paligemma-16',\n",
       " 'paligemma-17',\n",
       " 'paligemma-18',\n",
       " 'paligemma-19',\n",
       " 'paligemma-20',\n",
       " 'paligemma-21',\n",
       " 'paligemma-22',\n",
       " 'paligemma-23',\n",
       " 'paligemma-24',\n",
       " 'paligemma-25',\n",
       " 'paligemma-26',\n",
       " 'paligemma-27',\n",
       " 'paligemma-28',\n",
       " 'paligemma-29',\n",
       " 'paligemma-30',\n",
       " 'paligemma-31',\n",
       " 'paligemma-32',\n",
       " 'paligemma-33',\n",
       " 'paligemma-34',\n",
       " 'paligemma-35',\n",
       " 'paligemma-36',\n",
       " 'paligemma-37',\n",
       " 'paligemma-38',\n",
       " 'paligemma-39',\n",
       " 'paligemma-40',\n",
       " 'paligemma-41',\n",
       " 'paligemma-42',\n",
       " 'paligemma-43',\n",
       " 'paligemma-44',\n",
       " 'paligemma-45',\n",
       " 'paligemma-46',\n",
       " 'paligemma-47',\n",
       " 'paligemma-48',\n",
       " 'paligemma-49',\n",
       " 'paligemma-50',\n",
       " 'paligemma-51',\n",
       " 'paligemma-52',\n",
       " 'paligemma-53',\n",
       " 'paligemma-54',\n",
       " 'paligemma-55',\n",
       " 'paligemma-56',\n",
       " 'paligemma-57',\n",
       " 'paligemma-58',\n",
       " 'timesfm-1',\n",
       " 'timesfm-2',\n",
       " 'timesfm-3',\n",
       " 'timesfm-4',\n",
       " 'timesfm-5',\n",
       " 'timesfm-6',\n",
       " 'timesfm-7',\n",
       " 'timesfm-8',\n",
       " 'timesfm-9',\n",
       " 'timesfm-10',\n",
       " 'timesfm-11',\n",
       " 'timesfm-12',\n",
       " 'timesfm-13']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store = Chroma(\n",
    "    collection_name=index_name,\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=path_db.as_posix()\n",
    ")\n",
    "\n",
    "\n",
    "vector_store.add_documents(documents=lst_docs_corpus, ids=uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5029a39-3762-4c1a-8cf6-dd8642ab4e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Lag-Llama\n",
      "number of series is useful when sampling random windows\n",
      "from the pretraining corpus. Furth [{'extension': '.pdf', 'page': 5, 'source': 'lagllama'}]\n",
      "\n",
      "* Lag-Llama\n",
      "Figure 11: Lag-Llama fine-tuned forecasting examples on the downstream Requests Minute dat [{'extension': '.pdf', 'page': 20, 'source': 'lagllama'}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search(\n",
    "    \"how many datasets have been bundled to train lag llama\",\n",
    "    k=2,\n",
    ")\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content[:100]} [{res.metadata}]\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fdd359ec-988f-460d-8ef0-269a20dc2cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import chain\n",
    "\n",
    "chain_basic_rag = chain.rag_basic_with_sources(ChatOpenAI() , vector_store.as_retriever())\n",
    "response = chain_basic_rag.invoke({'input': \"how many datasets are bundle to train lag llama?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6fdb741c-e2e1-4258-b238-b8e015f91f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input', 'context', 'answer'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "40ca06a0-e371-4bfd-af13-eb951527fc17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper</th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LagLLama</td>\n",
       "      <td>What is the main goal of the LagLLama project?</td>\n",
       "      <td>The LagLLama project aims to optimize large-sc...</td>\n",
       "      <td>Introduction, Abstract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LagLLama</td>\n",
       "      <td>How does LagLLama achieve improved performance...</td>\n",
       "      <td>LagLLama achieves this by applying data augmen...</td>\n",
       "      <td>Section 2.3, Section 4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      paper                                           question  \\\n",
       "0  LagLLama     What is the main goal of the LagLLama project?   \n",
       "1  LagLLama  How does LagLLama achieve improved performance...   \n",
       "\n",
       "                                        ground_truth                  source  \n",
       "0  The LagLLama project aims to optimize large-sc...  Introduction, Abstract  \n",
       "1  LagLLama achieves this by applying data augmen...  Section 2.3, Section 4  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_eval_qs = pd.read_csv(mle_utils.path_data_raw / \"eval-questions\" / \"ai-papers.csv\")\n",
    "df_eval_qs.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "35a560dd-6a9e-4d66-805b-529b409c7a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'contexts', 'ground_truth'],\n",
       "    num_rows: 25\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset eval.populate_eval_dataset(df_eval_qs, chain_basic_rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4d981bac-fc69-4a06-9190-93d1dcc664bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  81%|██████████████████████████████▋       | 101/125 [02:01<00:28,  1.20s/it]\n",
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\manua\\.conda\\envs\\rag-advanced\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\manua\\.conda\\envs\\rag-advanced\\lib\\site-packages\\ragas\\executor.py\", line 95, in run\n",
      "    results = self.loop.run_until_complete(self._aresults())\n",
      "  File \"C:\\Users\\manua\\.conda\\envs\\rag-advanced\\lib\\asyncio\\base_events.py\", line 649, in run_until_complete\n",
      "    return future.result()\n",
      "  File \"C:\\Users\\manua\\.conda\\envs\\rag-advanced\\lib\\site-packages\\ragas\\executor.py\", line 83, in _aresults\n",
      "    raise e\n",
      "  File \"C:\\Users\\manua\\.conda\\envs\\rag-advanced\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"C:\\Users\\manua\\.conda\\envs\\rag-advanced\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"C:\\Users\\manua\\.conda\\envs\\rag-advanced\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"C:\\Users\\manua\\.conda\\envs\\rag-advanced\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"C:\\Users\\manua\\.conda\\envs\\rag-advanced\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"C:\\Users\\manua\\.conda\\envs\\rag-advanced\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"C:\\Users\\manua\\.conda\\envs\\rag-advanced\\lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 266, in _ascore\n",
      "    nli_result = await self.llm.generate(\n",
      "  File \"C:\\Users\\manua\\.conda\\envs\\rag-advanced\\lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "  File \"C:\\Users\\manua\\.conda\\envs\\rag-advanced\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "  File \"C:\\Users\\manua\\.conda\\envs\\rag-advanced\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "  File \"C:\\Users\\manua\\.conda\\envs\\rag-advanced\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "  File \"C:\\Users\\manua\\.conda\\envs\\rag-advanced\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "  File \"C:\\Users\\manua\\.conda\\envs\\rag-advanced\\lib\\site-packages\\tenacity\\__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "  File \"C:\\Users\\manua\\.conda\\envs\\rag-advanced\\lib\\site-packages\\tenacity\\__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"C:\\Users\\manua\\.conda\\envs\\rag-advanced\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"C:\\Users\\manua\\.conda\\envs\\rag-advanced\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"C:\\Users\\manua\\.conda\\envs\\rag-advanced\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\manua\\.conda\\envs\\rag-advanced\\lib\\site-packages\\ragas\\llms\\base.py\", line 170, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "  File \"C:\\Users\\manua\\.conda\\envs\\rag-advanced\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 570, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "  File \"C:\\Users\\manua\\.conda\\envs\\rag-advanced\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 530, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"C:\\Users\\manua\\.conda\\envs\\rag-advanced\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 715, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "  File \"C:\\Users\\manua\\.conda\\envs\\rag-advanced\\lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 623, in _agenerate\n",
      "    response = await self.async_client.create(messages=message_dicts, **params)\n",
      "  File \"C:\\Users\\manua\\.conda\\envs\\rag-advanced\\lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1412, in create\n",
      "    return await self._post(\n",
      "  File \"C:\\Users\\manua\\.conda\\envs\\rag-advanced\\lib\\site-packages\\openai\\_base_client.py\", line 1816, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "  File \"C:\\Users\\manua\\.conda\\envs\\rag-advanced\\lib\\site-packages\\openai\\_base_client.py\", line 1510, in request\n",
      "    return await self._request(\n",
      "  File \"C:\\Users\\manua\\.conda\\envs\\rag-advanced\\lib\\site-packages\\openai\\_base_client.py\", line 1596, in _request\n",
      "    return await self._retry_request(\n",
      "  File \"C:\\Users\\manua\\.conda\\envs\\rag-advanced\\lib\\site-packages\\openai\\_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "  File \"C:\\Users\\manua\\.conda\\envs\\rag-advanced\\lib\\site-packages\\openai\\_base_client.py\", line 1596, in _request\n",
      "    return await self._retry_request(\n",
      "  File \"C:\\Users\\manua\\.conda\\envs\\rag-advanced\\lib\\site-packages\\openai\\_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "  File \"C:\\Users\\manua\\.conda\\envs\\rag-advanced\\lib\\site-packages\\openai\\_base_client.py\", line 1611, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-afjHBubMyM5wopTheVicwv8G on tokens per min (TPM): Limit 200000, Used 197832, Requested 5385. Please try again in 965ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "ename": "ExceptionInRunner",
     "evalue": "The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mExceptionInRunner\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m     faithfulness,\n\u001b[0;32m      4\u001b[0m     answer_relevancy,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m     context_precision,\n\u001b[0;32m      8\u001b[0m )\n\u001b[1;32m---> 10\u001b[0m df_eval_res \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext_relevancy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext_precision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext_recall\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfaithfulness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43manswer_relevancy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\rag-advanced\\lib\\site-packages\\ragas\\evaluation.py:250\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(dataset, metrics, llm, embeddings, callbacks, in_ci, is_async, run_config, raise_exceptions, column_map)\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evaluation_group_cm\u001b[38;5;241m.\u001b[39mended:\n\u001b[0;32m    248\u001b[0m         evaluation_rm\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    252\u001b[0m     result \u001b[38;5;241m=\u001b[39m Result(\n\u001b[0;32m    253\u001b[0m         scores\u001b[38;5;241m=\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_list(scores),\n\u001b[0;32m    254\u001b[0m         dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[0;32m    255\u001b[0m         binary_columns\u001b[38;5;241m=\u001b[39mbinary_metrics,\n\u001b[0;32m    256\u001b[0m     )\n",
      "File \u001b[1;32m~\\.conda\\envs\\rag-advanced\\lib\\site-packages\\ragas\\evaluation.py:232\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(dataset, metrics, llm, embeddings, callbacks, in_ci, is_async, run_config, raise_exceptions, column_map)\u001b[0m\n\u001b[0;32m    230\u001b[0m results \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39mresults()\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;241m==\u001b[39m []:\n\u001b[1;32m--> 232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ExceptionInRunner()\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# convert results to dataset_like\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset):\n",
      "\u001b[1;31mExceptionInRunner\u001b[0m: The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead."
     ]
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "df_eval_res = evaluate(\n",
    "    dataset=dataset,\n",
    "    metrics=[\n",
    "        context_relevancy,\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "    ],\n",
    "    llm=llm,\n",
    "    embeddings=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9b7b9a9d-f968-449c-9848-f60b064e3fc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_eval_res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf_eval_res\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_eval_res' is not defined"
     ]
    }
   ],
   "source": [
    "df_eval_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bc9e08-f474-4a1e-a7e5-27dca5af81d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
